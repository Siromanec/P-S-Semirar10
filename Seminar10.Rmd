---
title: 'Seminar 10: Descriptive statistics'
output: html_notebook
---

## 1. Data (Id is the average of the member's no. in the P&S list)

### Task: Generate your data

This is a sample of size $n=1000$ combined from two samples of size $500$ from the normal distributions $N(\mu_k,\sigma_k^2)$ with $\mu_1 = Id$, $\sigma_1 = 3$ and $\mu_2 = -2$, $\sigma_2 = Id$

Hint: you can use $\texttt{mean = c}(\mu_1,\mu_2)$ and $\texttt{sd = c}(\sigma_1,\sigma_2)$

```{r}
id = (35+67+1)/3
# your code here
n <- 1000
sigma_1 <- 3
sigma_2 <- id
mu_1 <- id
mu_2 <- -2
mean <- c(mu_1, mu_2)
sd <- c(sigma_1, sigma_2)
my.data <- rnorm(n, mean, sd)
summary(my.data)
```

## 2. Visualise the data

### Task: Draw the histogram of the data, empirical density, and empirical cdf. Comment on whether the data are close to a normal one

```{r}
# draw histogram
hist(my.data, breaks = 30, probability = T)
# superimpose empirical density
plot(density(my.data))
# superimpose normal density
x <- seq(min(my.data), max(my.data), by = .01)
lines(x, dnorm(x, mean = mean(my.data), sd = sd(my.data)), col = "red", lwd = 2)
# plot(ecdf(my.data))
# lines(my.data, pnorm(my.data, mean = mean(my.data), sd = sd(my.data)), col ="red", lwd = 2)
```

### Plot ecdf

```{r}
# plot ecdf
plot(ecdf(my.data))
# superimpose cdf for standard normal
x <- seq(min(my.data), max(my.data), by = .01)
lines(x, pnorm(x, mean = mean(my.data), sd = sd(my.data)), col = "red", lwd = 2)

# calculate maximal difference
max(abs(ecdf(my.data)(x) - pnorm(x, mean = mean(my.data), sd = sd(my.data))))
```

## 3. Skewness and kurtosis of the data

**Skewness** is defined as $$
  \mathsf{E} \left[\left({\frac {X-\mu }{\sigma }}\right)^{3}\right]={\frac {\mu _{3}}{\sigma ^{3}}}={\frac {\mathsf{E} \left[(X-\mu )^{3}\right]}{\ \ \ (\mathsf{E} \left[(X-\mu )^{2}\right])^{3/2}}}
$$

Positive skewness means the data have longer right tail and then the mean value $\mathsf{E}(X)$ is greater than the median

**Kurtosis** is defined as $$
  \mathsf{E} \left[\left({\frac {X-\mu }{\sigma }}\right)^{4}\right]={\frac {\mu _{4}}{\sigma ^{4}}}={\frac {\mathsf{E} \left[(X-\mu )^{4}\right]}{\ \ \ (\mathsf{E} \left[(X-\mu )^{2}\right])^{2}}}
$$

For a standard normal distribution, the kurtosis is $3$. If we get a larger value, then more data are concentrated ner the mean, and the empirical density is more steep

```{r}
# skewness
skewness <- function(x) {
  m3 <- mean((x - mean(x))^3)
  m2 <- mean((x - mean(x))^2)
  return(m3 / m2^(3 / 2))
}

skewness_calculated <- skewness(my.data)
print(skewness_calculated)
if (skewness_calculated > 0) {
  print("The data are positively skewed - the tail is longer on the right side")
} else if (skewness_calculated < 0) {
  print("The data are negatively skewed - the tail is longer on the left side")
} else {
  print("The data are symmetrically distributed")
}

# kurtosis
kurtosis <- function(x) {
  m4 <- mean((x - mean(x))^4)
  m2 <- mean((x - mean(x))^2)
  return(m4 / m2^2)
}

kurtosis_calculated <- kurtosis(my.data)
print(kurtosis_calculated)
if (kurtosis_calculated > 3) {
  print("The data is more concentrated near the mean")
} else if (kurtosis_calculated < 3) {
  print("The data is more dispersed")
} else {
  print("The data is normally distributed")
}
```

## 4. Percentiles

### For q= 0.1,...,0.9 calculate sample q-percentiles manually (i.e. by sorting the data and picking the corresponding values) and by using percentile

```{r}
# sort the data
sorted_data <- sort(my.data)

# calculate sample q-percentiles manually
q <- seq(0.1, 0.9, 0.1)
print(q)
percentiles <- q * n
percentiles <- round(percentiles)
percentiles <- percentiles + 1
percentiles <- sorted_data[percentiles]
print(percentiles)

# calculate sample q-percentiles by using percentile
quantile <- quantile(my.data, q)
print(quantile)
```

## 5. Sample mean and sample standard error vs the theoretical ones

### What is the theoretical expected value of the random variable considered? What is the variance? Compare them to the sample values; explain the difference in the variance

$$ \mu_{th} = E(W|W=x)*P(W=x) + E(W|W=y)*P(W=y)  =\frac{k*E(X) + k*E(Y)}{2k} = \frac{E(X) +E(Y)}{2} = 0.5(\mu_1 + \mu_2)$$

$$ Var_{th} = Var(W) = E(W^2) - E(W)^2 $$
$$ E(W^2) = E(W^2|W=x)*P(W=x) + E(W^2|W=y)*P(W=y) =0.5*(E(X^2) + E(Y^2)) = 0.5*(Var(X)  + E(X)^2 + Var(X)  + E(X)^2)$$
$$E(W^2) =0.5(\sigma_1^2 + \sigma_2^2 + \mu_1^2 + \mu_2^2) $$
$$ Var_{th} = Var(W) = E(W^2) - E(W)^2 = 0.5(\sigma_1^2 + \sigma_2^2 + \mu_1^2 + \mu_2^2) - \frac{\mu_1^2 + \mu_2^2 + 2\mu_1\mu_2}{4}$$

$$ Var_{th} = 0.5(\sigma_1^2 + \sigma_2^2) + (\mu_1 - \mu_2)^2/4$$

```{r}
# your code here
# theoretical expected value and variance
theorexp <- sum(mean / 2)
theorexp
theorvar <- (sigma_1**2 + sigma_2**2) / 2 + 0.25 * (mu_1 - mu_2)**2
theorvar
# sample mean and sample variance
mean(my.data)
var(my.data)
```

## 6. k-sigma rule

### Calculate the fraction of the data that are within $k\sigma$ of the sample mean for $k=1,2,3$. Do we get the result expected? Why or why not?

```{r}
# Calculate the fraction of the data that are within $k\sigma$ of the sample mean for $k=1,2,3$. Do we get the result expected? Why or why not?

m <- mean(my.data)
s <- sd(my.data)
mean(abs(my.data - m) < s)
mean(abs(my.data - m) < s * 2)
mean(abs(my.data - m) < s * 3)
```
